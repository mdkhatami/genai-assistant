# GenAI Assistant Environment Configuration
# Copy this file to .env and fill in your actual values
# DO NOT commit the .env file to version control

# =============================================================================
# Authentication & Security
# =============================================================================

# JWT Configuration
# Generate a secure secret key with: openssl rand -hex 32
JWT_SECRET_KEY=your-secret-key-change-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Admin User Configuration
# IMPORTANT: Change these default credentials in production!
ADMIN_USERNAME=admin
ADMIN_PASSWORD=change-this-password-in-production
ADMIN_EMAIL=admin@example.com
ADMIN_FULL_NAME=Administrator

# =============================================================================
# API Keys (Required for respective features)
# =============================================================================

# OpenAI API Key (required for OpenAI LLM features)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Hugging Face Token (required for image generation)
# Get your token from: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=your_huggingface_token_here

# =============================================================================
# Server Configuration
# =============================================================================

# Server host and port
HOST=0.0.0.0
PORT=5000
DEBUG=false
LOG_LEVEL=INFO

# CORS Configuration
# Comma-separated list of allowed origins for CORS
# For production, specify exact origins (e.g., "https://yourdomain.com,https://app.yourdomain.com")
# For development, defaults to localhost origins
CORS_ORIGINS=http://localhost:8080,http://localhost:3000,http://127.0.0.1:8080,http://127.0.0.1:3000

# =============================================================================
# Port Configuration
# =============================================================================
# All port numbers should be configured here - no hardcoded ports in the codebase

# Main FastAPI server port
WEB_PORT=5000

# Standalone webapp HTTP server port
WEBAPP_PORT=8080

# Ollama service port
OLLAMA_PORT=11434

# Nginx ports (for production deployment)
NGINX_HTTP_PORT=80
NGINX_HTTPS_PORT=443

# =============================================================================
# Device Configuration (GPU/CPU)
# =============================================================================

# Device Preference: "auto" (default), "gpu"/"cuda", or "cpu"
# - auto: Automatically detect and use GPU if available, fall back to CPU
# - gpu/cuda: Prefer GPU, warn if unavailable
# - cpu: Force CPU mode
DEVICE_PREFERENCE=auto

# Optional: Restrict which GPUs are visible to the application
# Only set this if you want to limit GPU visibility (e.g., multi-tenant systems)
# Leave commented out to use all available GPUs
# CUDA_VISIBLE_DEVICES=0,1,2,3

# Optional: Override device for specific services (rarely needed)
# These override the global DEVICE_PREFERENCE for individual services
# Leave commented out to use DEVICE_PREFERENCE for all services
# IMAGE_GENERATION_DEVICE=auto
# TRANSCRIPTION_DEVICE=auto

# =============================================================================
# Ollama Configuration
# =============================================================================

# Ollama base URL
# OLLAMA_BASE_URL can be set directly, or will be constructed from OLLAMA_PORT
# If OLLAMA_BASE_URL is not set, it defaults to http://localhost:${OLLAMA_PORT}
OLLAMA_BASE_URL=http://localhost:11434
