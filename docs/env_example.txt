# GenAI Assistant Environment Configuration
# 
# NOTE: This file is kept for backward compatibility. The primary template is .env_example in the project root.
# Please use the root .env_example file as the main reference.
#
# Copy this file to .env in the project root and fill in your actual values
# DO NOT commit the .env file to version control

# =============================================================================
# Authentication & Security
# =============================================================================

# JWT Configuration
# Generate a secure secret key with: openssl rand -hex 32
JWT_SECRET_KEY=your-secret-key-change-in-production
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Admin User Configuration
# IMPORTANT: Change these default credentials in production!
ADMIN_USERNAME=admin
ADMIN_PASSWORD=change-this-password-in-production
ADMIN_EMAIL=admin@example.com
ADMIN_FULL_NAME=Administrator

# =============================================================================
# API Keys (Required for respective features)
# =============================================================================

# OpenAI API Key (required for OpenAI LLM features)
# Get your key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Hugging Face Token (required for image generation)
# Get your token from: https://huggingface.co/settings/tokens
HUGGINGFACE_TOKEN=your_huggingface_token_here

# =============================================================================
# Server Configuration
# =============================================================================

# Server host and port
HOST=0.0.0.0
PORT=5000
DEBUG=false
LOG_LEVEL=INFO

# CORS Configuration
# Comma-separated list of allowed origins for CORS
# For production, specify exact origins (e.g., "https://yourdomain.com,https://app.yourdomain.com")
# For development, defaults to localhost origins
CORS_ORIGINS=http://localhost:8080,http://localhost:3000,http://127.0.0.1:8080,http://127.0.0.1:3000

# =============================================================================
# Port Configuration
# =============================================================================
# All port numbers should be configured here - no hardcoded ports in the codebase

# Main FastAPI server port
WEB_PORT=5000

# Standalone webapp HTTP server port
WEBAPP_PORT=8080

# Ollama service port
OLLAMA_PORT=11434

# Nginx ports (for production deployment)
NGINX_HTTP_PORT=80
NGINX_HTTPS_PORT=443

# =============================================================================
# GPU Configuration
# =============================================================================

# CUDA visible devices (comma-separated GPU indices, e.g., "0,1,2,3")
# Leave empty or unset to use all available GPUs
CUDA_VISIBLE_DEVICES=1,2,3

# Optional: Service-specific GPU overrides
# These override CUDA_VISIBLE_DEVICES for specific services
OLLAMA_GPU_INDEX=1
TRANSCRIPTION_GPU_INDEX=2
IMAGE_GENERATION_GPU_INDEX=3

# =============================================================================
# Ollama Configuration
# =============================================================================

# Ollama base URL
# OLLAMA_BASE_URL can be set directly, or will be constructed from OLLAMA_PORT
# If OLLAMA_BASE_URL is not set, it defaults to http://localhost:${OLLAMA_PORT}
OLLAMA_BASE_URL=http://localhost:11434
